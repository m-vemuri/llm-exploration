{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d658f909-e679-41e9-9c4e-e0241c719049",
   "metadata": {},
   "source": [
    "If you're not running in Saturn Cloud, you need to install these libraries:\n",
    "\n",
    "Make sure you use the latest versions\n",
    "\n",
    "```\n",
    "pip install -U transformers accelerate bitsandbytes\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "250e5960-5d4a-4dc8-b889-7bac67824ffd",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-07-06T00:28:37.271575Z",
     "iopub.status.busy": "2024-07-06T00:28:37.271199Z",
     "iopub.status.idle": "2024-07-06T00:28:37.278050Z",
     "shell.execute_reply": "2024-07-06T00:28:37.277356Z",
     "shell.execute_reply.started": "2024-07-06T00:28:37.271550Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ['HF_HOME'] = '/run/cache'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "506fab2a-a50c-42bd-a106-c83a9d2828ea",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-07-06T00:28:37.279319Z",
     "iopub.status.busy": "2024-07-06T00:28:37.279046Z",
     "iopub.status.idle": "2024-07-06T00:28:38.565483Z",
     "shell.execute_reply": "2024-07-06T00:28:38.564643Z",
     "shell.execute_reply.started": "2024-07-06T00:28:37.279299Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--2024-07-06 00:28:38--  https://raw.githubusercontent.com/alexeygrigorev/minsearch/main/minsearch.py\n",
      "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.111.133, 185.199.109.133, 185.199.110.133, ...\n",
      "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.111.133|:443... connected.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: 3832 (3.7K) [text/plain]\n",
      "Saving to: ‘minsearch.py’\n",
      "\n",
      "minsearch.py        100%[===================>]   3.74K  --.-KB/s    in 0s      \n",
      "\n",
      "2024-07-06 00:28:38 (85.6 MB/s) - ‘minsearch.py’ saved [3832/3832]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "!rm -f minsearch.py\n",
    "!wget https://raw.githubusercontent.com/alexeygrigorev/minsearch/main/minsearch.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "3ac947de-effd-4b61-8792-a6d7a133f347",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-07-06T00:28:38.567370Z",
     "iopub.status.busy": "2024-07-06T00:28:38.566937Z",
     "iopub.status.idle": "2024-07-06T00:28:39.776162Z",
     "shell.execute_reply": "2024-07-06T00:28:39.775348Z",
     "shell.execute_reply.started": "2024-07-06T00:28:38.567334Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<minsearch.Index at 0x7fe9f28d4ee0>"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import requests \n",
    "import minsearch\n",
    "\n",
    "docs_url = 'https://github.com/DataTalksClub/llm-zoomcamp/blob/main/01-intro/documents.json?raw=1'\n",
    "docs_response = requests.get(docs_url)\n",
    "documents_raw = docs_response.json()\n",
    "\n",
    "documents = []\n",
    "\n",
    "for course in documents_raw:\n",
    "    course_name = course['course']\n",
    "\n",
    "    for doc in course['documents']:\n",
    "        doc['course'] = course_name\n",
    "        documents.append(doc)\n",
    "\n",
    "index = minsearch.Index(\n",
    "    text_fields=[\"question\", \"text\", \"section\"],\n",
    "    keyword_fields=[\"course\"]\n",
    ")\n",
    "\n",
    "index.fit(documents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "8f087272-b44d-4738-9ea2-175ec63a058b",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-07-06T00:28:39.778821Z",
     "iopub.status.busy": "2024-07-06T00:28:39.778167Z",
     "iopub.status.idle": "2024-07-06T00:28:39.782455Z",
     "shell.execute_reply": "2024-07-06T00:28:39.781809Z",
     "shell.execute_reply.started": "2024-07-06T00:28:39.778787Z"
    }
   },
   "outputs": [],
   "source": [
    "def search(query):\n",
    "    boost = {'question': 3.0, 'section': 0.5}\n",
    "\n",
    "    results = index.search(\n",
    "        query=query,\n",
    "        filter_dict={'course': 'data-engineering-zoomcamp'},\n",
    "        boost_dict=boost,\n",
    "        num_results=5\n",
    "    )\n",
    "\n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "742ab881-499a-4675-83c4-2013ea1377b9",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-07-06T00:28:39.784114Z",
     "iopub.status.busy": "2024-07-06T00:28:39.783508Z",
     "iopub.status.idle": "2024-07-06T00:28:39.789979Z",
     "shell.execute_reply": "2024-07-06T00:28:39.789396Z",
     "shell.execute_reply.started": "2024-07-06T00:28:39.784081Z"
    }
   },
   "outputs": [],
   "source": [
    "def build_prompt(query, search_results):\n",
    "    prompt_template = \"\"\"\n",
    "You're a course teaching assistant. Answer the QUESTION based on the CONTEXT from the FAQ database.\n",
    "Use only the facts from the CONTEXT when answering the QUESTION.\n",
    "\n",
    "QUESTION: {question}\n",
    "\n",
    "CONTEXT: \n",
    "{context}\n",
    "\"\"\".strip()\n",
    "\n",
    "    context = \"\"\n",
    "    \n",
    "    for doc in search_results:\n",
    "        context = context + f\"section: {doc['section']}\\nquestion: {doc['question']}\\nanswer: {doc['text']}\\n\\n\"\n",
    "    \n",
    "    prompt = prompt_template.format(question=query, context=context).strip()\n",
    "    return prompt\n",
    "\n",
    "def llm(prompt):\n",
    "    response = client.chat.completions.create(\n",
    "        model='gpt-4o',\n",
    "        messages=[{\"role\": \"user\", \"content\": prompt}]\n",
    "    )\n",
    "    \n",
    "    return response.choices[0].message.content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "fe8bff3e-b672-42be-866b-f2d9bb217106",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-07-06T00:28:39.791266Z",
     "iopub.status.busy": "2024-07-06T00:28:39.790876Z",
     "iopub.status.idle": "2024-07-06T00:28:39.795598Z",
     "shell.execute_reply": "2024-07-06T00:28:39.794885Z",
     "shell.execute_reply.started": "2024-07-06T00:28:39.791235Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def rag(query):\n",
    "    search_results = search(query)\n",
    "    prompt = build_prompt(query, search_results)\n",
    "    answer = llm(prompt)\n",
    "    return answer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "091a77e6-936b-448e-a04b-bad1001f5bb0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "abc9655c-4f4c-438f-bfb6-77b0b27023eb",
   "metadata": {},
   "source": [
    "# Phi-3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "2aa05338-10c8-4e32-89ae-744f972f7a09",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-07-06T00:34:12.054978Z",
     "iopub.status.busy": "2024-07-06T00:34:12.054583Z",
     "iopub.status.idle": "2024-07-06T00:34:12.099202Z",
     "shell.execute_reply": "2024-07-06T00:34:12.098322Z",
     "shell.execute_reply.started": "2024-07-06T00:34:12.054953Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Filesystem      Size  Used Avail Use% Mounted on\n",
      "overlay         100G   54G   47G  54% /\n",
      "tmpfs            64M     0   64M   0% /dev\n",
      "tmpfs           7.7G     0  7.7G   0% /sys/fs/cgroup\n",
      "/dev/nvme0n1p1  100G   54G   47G  54% /run\n",
      "tmpfs            14G  4.0K   14G   1% /dev/shm\n",
      "/dev/nvme2n1    2.0G  131M  1.8G   7% /home/jovyan\n",
      "tmpfs            14G  120K   14G   1% /home/jovyan/.saturn\n",
      "tmpfs            14G   12K   14G   1% /run/secrets/kubernetes.io/serviceaccount\n",
      "tmpfs           7.7G   12K  7.7G   1% /proc/driver/nvidia\n",
      "tmpfs           7.7G  4.7M  7.7G   1% /run/nvidia-persistenced/socket\n",
      "tmpfs           7.7G     0  7.7G   0% /proc/acpi\n",
      "tmpfs           7.7G     0  7.7G   0% /sys/firmware\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    }
   ],
   "source": [
    "%%sh \n",
    "\n",
    "df -h"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "215130fd-7f5e-45b6-8de8-1b0f92569545",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-07-06T00:34:26.439037Z",
     "iopub.status.busy": "2024-07-06T00:34:26.438656Z",
     "iopub.status.idle": "2024-07-06T00:34:26.481330Z",
     "shell.execute_reply": "2024-07-06T00:34:26.480440Z",
     "shell.execute_reply.started": "2024-07-06T00:34:26.439012Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total 4.0K\n",
      "drwxr-sr-x 5 jovyan jovyan 124 Jul  6 00:29 .\n",
      "drwxr-sr-x 4 jovyan jovyan  32 Jul  6 00:29 ..\n",
      "drwxr-sr-x 4 jovyan jovyan  91 Jul  6 00:29 .locks\n",
      "drwxr-sr-x 6 jovyan jovyan  65 Jul  6 00:04 models--google--flan-t5-xl\n",
      "drwxr-sr-x 6 jovyan jovyan  65 Jul  6 00:31 models--microsoft--Phi-3-mini-128k-instruct\n",
      "-rw-r--r-- 1 jovyan jovyan   1 Jul  6 00:01 version.txt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    }
   ],
   "source": [
    "%%sh \n",
    "\n",
    "ls -alh /run/cache/hub/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "4f57f7f0-a472-4d49-80b5-b79bba43036f",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-07-06T00:34:21.257777Z",
     "iopub.status.busy": "2024-07-06T00:34:21.257313Z",
     "iopub.status.idle": "2024-07-06T00:34:21.262126Z",
     "shell.execute_reply": "2024-07-06T00:34:21.261470Z",
     "shell.execute_reply.started": "2024-07-06T00:34:21.257748Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/run/cache/hub\n"
     ]
    }
   ],
   "source": [
    "from transformers import TRANSFORMERS_CACHE\n",
    "print(TRANSFORMERS_CACHE)\n",
    "\n",
    "# import shutil\n",
    "# shutil.rmtree(TRANSFORMERS_CACHE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "43350654-0c86-4b68-b395-2962d893822a",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-07-06T00:29:44.218942Z",
     "iopub.status.busy": "2024-07-06T00:29:44.218542Z",
     "iopub.status.idle": "2024-07-06T00:30:31.948951Z",
     "shell.execute_reply": "2024-07-06T00:30:31.948334Z",
     "shell.execute_reply.started": "2024-07-06T00:29:44.218915Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6121a6009921411b8db8bfb0f5c05878",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/3.48k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b282e9ecc7c943f79a0394a53ab1a189",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "configuration_phi3.py:   0%|          | 0.00/11.2k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "A new version of the following files was downloaded from https://huggingface.co/microsoft/Phi-3-mini-128k-instruct:\n",
      "- configuration_phi3.py\n",
      ". Make sure to double-check they do not contain any added malicious code. To avoid downloading new versions of the code file, you can pin a revision.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "330653af63624f518a97b7cef4324e05",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "modeling_phi3.py:   0%|          | 0.00/73.2k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "A new version of the following files was downloaded from https://huggingface.co/microsoft/Phi-3-mini-128k-instruct:\n",
      "- modeling_phi3.py\n",
      ". Make sure to double-check they do not contain any added malicious code. To avoid downloading new versions of the code file, you can pin a revision.\n",
      "`flash-attention` package not found, consider installing for better performance: No module named 'flash_attn'.\n",
      "Current `flash-attention` does not support `window_size`. Either upgrade or use `attn_implementation='eager'`.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "34b3d02394314f36ac50f4cbb4ef5601",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors.index.json:   0%|          | 0.00/16.3k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0a8aab1f9ce6424ca41a9dd7a8b8fc65",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fa431eb8c796491d8268e9018137094c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model-00001-of-00002.safetensors:   0%|          | 0.00/4.97G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "761360f784fd48d4a9948574746fd1df",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model-00002-of-00002.safetensors:   0%|          | 0.00/2.67G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "30cf4e7623ad47ecab4dd8c8ae329b0b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b8f5a5a9645347df81f445d0913ac4b0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "generation_config.json:   0%|          | 0.00/181 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import torch \n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer, pipeline \n",
    "\n",
    "torch.random.manual_seed(0) \n",
    "model = AutoModelForCausalLM.from_pretrained( \n",
    "    \"microsoft/Phi-3-mini-128k-instruct\",  \n",
    "    device_map=\"cuda\",  \n",
    "    torch_dtype=\"auto\",  \n",
    "    trust_remote_code=True,  \n",
    ") \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "9acfd73f-454f-4a68-a75d-0200db443a18",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-07-06T00:31:00.717271Z",
     "iopub.status.busy": "2024-07-06T00:31:00.716744Z",
     "iopub.status.idle": "2024-07-06T00:31:01.274502Z",
     "shell.execute_reply": "2024-07-06T00:31:01.273793Z",
     "shell.execute_reply.started": "2024-07-06T00:31:00.717237Z"
    }
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7845f99a29674bbbb366d725ab39f792",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/3.44k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d9bf1f42ac3f439097f74103a9bce1e7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.model:   0%|          | 0.00/500k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0f7815433490441485cb178c6b85d4df",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json:   0%|          | 0.00/1.94M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "56023fc3a1ba425cacc9c6aae3eca88c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "added_tokens.json:   0%|          | 0.00/306 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2e7b8aad9f8f40b4ab1fa784ce1e6853",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "special_tokens_map.json:   0%|          | 0.00/599 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    }
   ],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(\"microsoft/Phi-3-mini-128k-instruct\") \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "8af6e986-ac3d-422a-8291-63842153c389",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-07-06T00:31:11.918798Z",
     "iopub.status.busy": "2024-07-06T00:31:11.918431Z",
     "iopub.status.idle": "2024-07-06T00:31:19.938613Z",
     "shell.execute_reply": "2024-07-06T00:31:19.937862Z",
     "shell.execute_reply.started": "2024-07-06T00:31:11.918772Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/saturncloud/envs/saturn/lib/python3.9/site-packages/transformers/generation/configuration_utils.py:540: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.0` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.\n",
      "  warnings.warn(\n",
      "The `seen_tokens` attribute is deprecated and will be removed in v4.41. Use the `cache_position` model input instead.\n",
      "You are not running the flash-attention implementation, expect numerical differences.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " To solve the equation 2x + 3 = 7, follow these steps:\n",
      "\n",
      "1. Subtract 3 from both sides of the equation:\n",
      "   2x + 3 - 3 = 7 - 3\n",
      "   2x = 4\n",
      "\n",
      "2. Divide both sides of the equation by 2:\n",
      "   2x/2 = 4/2\n",
      "   x = 2\n",
      "\n",
      "So, the solution to the equation 2x + 3 = 7 is x = 2.\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "messages = [ \n",
    "    {\"role\": \"system\", \"content\": \"You are a helpful AI assistant.\"}, \n",
    "    {\"role\": \"user\", \"content\": \"Can you provide ways to eat combinations of bananas and dragonfruits?\"}, \n",
    "    {\"role\": \"assistant\", \"content\": \"Sure! Here are some ways to eat bananas and dragonfruits together: 1. Banana and dragonfruit smoothie: Blend bananas and dragonfruits together with some milk and honey. 2. Banana and dragonfruit salad: Mix sliced bananas and dragonfruits together with some lemon juice and honey.\"}, \n",
    "    {\"role\": \"user\", \"content\": \"What about solving an 2x + 3 = 7 equation?\"}, \n",
    "] \n",
    "\n",
    "pipe = pipeline( \n",
    "    \"text-generation\", \n",
    "    model=model, \n",
    "    tokenizer=tokenizer, \n",
    ") \n",
    "\n",
    "generation_args = { \n",
    "    \"max_new_tokens\": 500, \n",
    "    \"return_full_text\": False, \n",
    "    \"temperature\": 0.0, \n",
    "    \"do_sample\": False, \n",
    "} \n",
    "\n",
    "output = pipe(messages, **generation_args) \n",
    "print(output[0]['generated_text'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "f4b2efb0-b367-4b8d-8ee4-31aaa81fbe0a",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-07-06T00:33:48.114989Z",
     "iopub.status.busy": "2024-07-06T00:33:48.114608Z",
     "iopub.status.idle": "2024-07-06T00:33:52.109902Z",
     "shell.execute_reply": "2024-07-06T00:33:52.109083Z",
     "shell.execute_reply.started": "2024-07-06T00:33:48.114961Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Yes, you can still join the course even if you discover it after the start date. You are still eligible to submit the homeworks, but remember to meet the deadlines for the final projects.'"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def rag_phi3(query):\n",
    "    \n",
    "    def llm_ph3(prompt):\n",
    "        messages = [\n",
    "        {\"role\": \"user\", \"content\": prompt},\n",
    "        ]\n",
    "\n",
    "        generation_args = {\n",
    "            \"max_new_tokens\": 500,\n",
    "            \"return_full_text\": False,\n",
    "            \"temperature\": 0.0,\n",
    "            \"do_sample\": False,\n",
    "        }\n",
    "\n",
    "        output = pipe(messages, **generation_args)\n",
    "        return output[0]['generated_text'].strip()\n",
    "\n",
    "    \n",
    "    search_results = search(query)\n",
    "    \n",
    "    \n",
    "    \n",
    "    prompt = build_prompt(query, search_results)\n",
    "    answer = llm_ph3(prompt)\n",
    "    return answer\n",
    "\n",
    "\n",
    "rag_phi3(\"I just discovered the course. Can I still join it?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d9ff628-74e5-458f-938c-6fe05a93ec0e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
